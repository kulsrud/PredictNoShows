---
title: "Predicting Medical Appointment No-shows"
author: "Knut Ulsrud"
date: "6/4/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load, echo=FALSE, include = FALSE}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org") #For general data cleaning
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org") #For creating test and train set
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org") #For displaying graphs side by side
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org") #For manipulating date variables
if(!require(woeBinning)) install.packages("WoeBinning", repos = "http://cran.us.r-project.org") #For binning variables automatically

#ML Packages
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org") #For running some caret functions
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org") #For producing ROC curves
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org") #For visualizing rpart plots
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org") #For rpart functions
if(!require(DMwR)) install.packages("DMwR", repos = "http://cran.us.r-project.org") #For running ml


url<- "https://github.com/kulsrud/PredictNoShows/raw/master/" 
file<- "KaggleV2-May-2016.csv"
df<- read.csv(paste(url, file, sep = ""))

#load("ml-2020-04-27.Rdata")

```

# Overview
This section presents the project's goals and a high-level overview of project data. 

## Goal of the project and key steps that were performed
This project's goal was to predict attendance (i. e. No-show vs. attended) for doctor's appointments based on publicly available medical appointment data. This report was produced as a submission to the Data Science: Capstone course provided by HarvardX on the EdX platform.

## Dataset Description
The medical appointment dataset  has 110,527 rows and 14 columns, and contains patient attendance data from April 29 to June 8, 2016, for a clinic in Brazil. The original dataset was sourced via Kaggle. https://www.kaggle.com/joniarroba/noshowappointments. Columns contain the following information:  

Variable name |Variable Description
--------------|------------------------------------------------------------------------
1. PatientId | Unique identifier for each patient
2. AppointmentID | Unique identifier for each appointment
3. Gender | Male or Female
4. ScheduledDay | When the appointment was scheduled
5. AppointmentDay | When the appointment happened
6. Age | Patient age
7. Neighbourhood | What neighbourhood patient lived in
8. Scholarship | Binary variable related to scholarship reception through the Bolsa Familia program
9. Hypertension | Whether the patient lived with hypertension
10. Diabetes | Whether the patient lived with diabetes
11. Alcoholism | Whether the patient lived with alcoholism
12. Handicap | Whether the patient lived with a physical handicap (i. e. blindness, inability to walk, etc.)
13. SMS_received | Whether the patient received an SMS reminder
14. No_show | Whether the patient showed up for their appointment



The following table shows the first four rows of the dataset.  

```{r description, echo=FALSE}

#Correct typos in variable names
names(df)[names(df)=="Hipertension"]<- "Hypertension"
names(df)[names(df)=="Handcap"]<- "Handicap"
names(df)[names(df)=="No.show"]<- "No_show"

#To show top 4 rows of sample data
knitr::kable(head(df, 4), "markdown") 

```

# Methods and Analysis
This section presents steps taken to perform initial analysis on the dataset. It includes the following sections: Data Preaparation, Data Exploration and Visualization, Insights Gained, and Modelling Approach.  

## Data Preparation
A high-level check revealed that there were no missing values in the appointment dataset. The data was already presented in a tidy format (one row equals one observation / appointment) which means no additional transformation was required to start using the dataset.

There are two date columns which were provided in year-month-date -- hour-minute-second format (i. e. "2016-04-29T00:00:00Z"). To facilitate analysis, both ScheduledDay and AppointmentDay columns were transformed into the following:

* Day of week
* Hour of day (only available for ScheduledDay)
* Duration from booking to appointment

For AppointmentDay all hour-minute-second data was set to 00:00:00. This means that calculated duration from booking to appointment might err by the number of hours from 00:00:00 to the actual appointment time. 

```{r cleaning, echo = FALSE, include = FALSE}

head(df) #The dataset is tidy, with each row representing one appointment slot 
colSums((is.na(df))) #There are no missing values in the dataset

#Transformed date variables to date format. 
df<- df %>% mutate(ScheduledDay = as_datetime(str_remove(str_replace(ScheduledDay, "T", " "), "Z")),
                      AppointmentDay = as_datetime(str_remove(str_replace(AppointmentDay, "T", " "), "Z")),
                      AppointmentWeekDay = weekdays(AppointmentDay),
                      ScheduledWeekDay = weekdays(ScheduledDay),
                      ScheduledHour = hour(ScheduledDay),
                      No_show = ifelse(No_show=="No", FALSE, TRUE),
                      BookingDifference = as.numeric(AppointmentDay - ScheduledDay))


```

## Data Exploration and Visualization
It is likely that there is systematic variation between patient specific information and whether or not they No-show. This section will explore this variation according to 

* Scheduling and appointment timing
* Past no-shows
* Demographics effects (i. e. age, gender, etc.)
* Medical information
* Other binary identifiers

It will also explore basic characteristics of variables of interest.

As shown in the table, roughly 20% of appointments in the dataset were missed.

```{r base, echo = FALSE}

#Estimate overall No-show proportion
knitr::kable(prop.table(table(df$No_show)), col.names = c("No-show", "Proportion"), format = "markdown", digits = 3)

#Highlight total duration of dataset
min_date<- min(df$AppointmentDay)
max_date<- max(df$AppointmentDay)
myDates <-seq(from = min_date, to = max_date, by = "days")
duration<- length(which(wday(myDates) %in% c(2:6)))
#Number of weekdays covered: 


```

 `r print(paste("The dataset covers", duration, "days, from", min_date, "to", max_date))`

### Appointment Timing
This section explores variation between time-related variables and appointment No-shows. 

#### Appointment and scheduling Weekday
At first glance it appears that appointments that were scheduled on Saturdays were much less likely to be missed. Otherwise, scheduling day does not appear to vary by meaningful proportions. 

```{r weekday, echo = FALSE, fig.align='center', fig.height=4}

#Look at No-shows by weekday
scheduled<- df %>%
     group_by(ScheduledWeekDay) %>% #To see individual movies by name
     summarize(No_show = mean(No_show)) %>% 
     ggplot(aes(factor(ScheduledWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"), ordered = TRUE), No_show))+
     geom_bar(stat = "identity")+
     coord_flip()+
    scale_y_continuous(limits = c(0, 0.25))+
     labs(title = "No-show proportion, by weekday scheduled",
       x = NULL, 
       y = NULL)

appointment<- df %>%
    group_by(AppointmentWeekDay) %>% #To see individual movies by name
     summarize(No_show = mean(No_show)) %>% 
      ggplot(aes(factor(AppointmentWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"), ordered = TRUE), No_show))+
      geom_bar(stat = "identity")+
      coord_flip()+
      scale_y_continuous(limits = c(0, 0.25))+
      labs(title = "No-show proportion, by appointment weekday",
       x = NULL, 
       y = NULL)

grid.arrange(scheduled, appointment)

```

On the other hand, appointents held on Saturdays appear to be missed more often than other days, while Thursday appointments are kept most often. There is minor variation between other days of the week.

Further exploration reveal that that much fewer appointments were scheduled and held on Saturdays than on other days. This indicates that Saturday data may be less reliable than other weekdays, as it may be susceptible to for instance selection bias. 

```{r saturdays, echo = FALSE, fig.align='center'}

#Scheduled on Saturdays vs. No-show
df %>% 
  group_by(factor(ScheduledWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  summarize(n = n(), No_show = mean(No_show)) %>%
  knitr::kable(digits = 2, col.names = c("Scheduled Weekday", "Appointments", "Percent No-show"))
```



```{r saturdays_3, echo = FALSE, fig.align='center'}

#Appointment on Saturdays vs. Walk-ins
df %>% 
  group_by(factor(AppointmentWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  summarize(n = n(), No_show = mean(No_show)) %>%
  knitr::kable(digits = 2, col.names = c("Appointment Weekday", "Appointments", "Percent No-show"))

```

Instead of filtering for Saturday appointments and removing useful data, a more prudent decision would be to drop Weekday Scheduled. 

Saturday appointment effects are small enough that it should not meaningfully skew the data, and the variable's importance to the final model could be tested. This would confirm whether the variation between appointment weekdays is explained by other variables. 

#### Hour of day scheduled
The hour of day that appointments were scheduled also seems to have an impact, varying from around 15% No-shows early in the morning, to over 30% late in the evening. It is unclear why this should be the case, other than that patients may be more tired and forgetful and therefore forget appointments scheduled later in the day. 

```{r hours, echo = FALSE, fig.align='center', fig.height=4, fig.width=6}

#Look at No-shows by hour of the day
df %>%
     group_by(ScheduledHour) %>% #To see individual movies by name
     summarize(No_show = mean(No_show)) %>% 
     ggplot(aes(ScheduledHour, No_show))+
        geom_bar(stat = "identity")+
  labs(title = "Appointment no-show proportion",
       x = "Hour of day scheduled", 
       y = "No-show proportion")

```

#### Difference between scheduling and appointment
By binning the difference between scheduled time and appointment time we see that there is substantial and systematic variation between time differences, and No-shows. 

Interestingly there are appointments that are recorded as scheduled after the appointment took place. This has two potential explanations. One is the aforementioned issue where exact appointment time was not reported, only date and 00:00:00 for hour-minute-second. This would mean that any same-day scheduled appointment would appear negative. Second, a patient walk-in could happen, and entered into the system by an administrator later. 

Either way, the recorded difference between scheduling and appointment timing deviates from the true difference by a number of hours. This is made clear by the presence of negative values. This paper continues the timing difference exploration with this caveat in mind. 

```{r wait, echo = FALSE, fig.align='center'}
#Bin difference between scheduled time and appointment time
df %>%
  group_by(BookDiffBin = ntile(BookingDifference, 12)) %>%
  summarize(No_show = mean(No_show), BookDiff = mean(BookingDifference)) %>%
  ggplot(aes(BookDiffBin, No_show))+
  geom_bar(stat = "identity", size = 1)+
  #geom_step(aes(ntile(BookDiffBin, 3)))
  geom_label(aes(x = BookDiffBin, y = No_show + 0.05, label = round(BookDiff, 0)))+
  scale_x_continuous(breaks = c(seq.int(1, 12, 1)))+
  labs(title = "Avg difference scheduling and appointment",
       x = "Bin no.", 
       y = "No-show proportion")

```

To do a high-level exploration of No-show outcomes it is reasonable to bin the hours difference data. This is especially the case given that individual observations might vary within a few hours from their actual time, as just explained. This would smooth data recording errors while retaining high-level variation. 

Binning was based on the intuition that three common appointment types vary by Walk-ins (less than 1 hours difference), Scheduled within a week (1 to 168 hours difference), and Scheduled within more than one week (more than 168 hours difference)


```{r bins, echo = FALSE, fig.align='center', fig.width=5, fig.height=3.5}


#Intuition about walk-ins
df %>%
  mutate(AppointmentType = ifelse(BookingDifference > 1, ifelse(BookingDifference > 168, "Long-term", "Within one week"), "Walk-in")) %>%
  group_by(AppointmentType) %>%
  summarize(No_show = mean(No_show), BookDiff = mean(BookingDifference)) %>%
  ggplot(aes(reorder(AppointmentType,No_show), No_show))+
  geom_bar(stat = "identity", size = 1)+
  geom_label(aes(x = AppointmentType, y = No_show + 0.05, label = round(BookDiff, 0)))+
  #scale_x_discrete(breaks = NULL)+
  scale_y_continuous(limits = c(0, 0.4))+
  #theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(title = "No-show likelihood, Binned by Appointment Type",
       x = "Appointment Type", 
       y = "Likelihood of No-show")


```

Binning shows that the intuition about appointment types is reasonable for the purposes of exploratory analysis. 

### Demographics 
This section explores variations in the No-show outcome by different demographic factors: 

* Patient effects 
* Age 
* Gender 
* City Neighbourhood patient lives in 

#### Patient effects
The scatterplot confirms the intution that there are differences in how many appointments patients had over the period of the dataset, and how likely they were to miss their apointments. For instance, there is a sharp cutoff in No-show likelihood around an average of one appointment per day. In other words, patients who see their doctor more frequently are more likely to keep their appointments. The observation that some patients have more than one appointment per day is reasonable. This is because one longer appointment would likely be scheduled as a sequence of individual appointments in medical scheduling software, giving the appearance of multiple appointments. 

```{r demographics, echo = FALSE, fig.align='center', fig.height=3, fig.width=5}

#Looking at patient effects
df %>%
  group_by(PatientId) %>%
  summarize(No_show = mean(No_show), 
            n = n(), 
            AppointmentsPerDay = n / duration) %>% #Assuming that multiple appointments per day means longer appointments (i. e. units scheduled back to back)
  ggplot(aes(AppointmentsPerDay, No_show))+
  geom_point()+
  labs(title = "No-show proportion by patient ID",
       x = "Average number of appointments per day", 
       y = "Average proportion No-shows")
  
```

#### Age
There is a clear visual relationship between age and likelihood of no-shows. Patients in their 20s seem to miss appointments more often than other age groups, followed by gradually lower chance of No-show as patients age. Older patients diverge from this pattern, where some patients are reliable, and others not. 

```{r age, echo = FALSE, warning=FALSE, fig.align='center', message=FALSE, fig.width=5, fig.height=4.5}

#Looking at age effects
df %>% 
  filter(Age < 100) %>% 
  group_by(Age) %>%
  summarize(No_show = mean(No_show), n = n()) %>% 
  ggplot(aes(Age, No_show))+
  geom_point()+
  geom_smooth(method = "auto")+
  labs(title = "No-show proportion by age",
       x = "Age", 
       y = "Average proportion No-shows")

```

#### Gender
There does not appear to be any obvious connections between gender and likelihood of No-shows. 

```{r gender, echo = FALSE, fig.align='center', fig.width=5, fig.height=3}

#Looking at Gender effects
df %>%
  group_by(Gender) %>%
  summarize(No_show = mean(No_show)) %>%
  ggplot(aes(Gender, No_show))+
  geom_bar(stat = "identity")+
  labs(title = "No-show proportion by gender",
       x = "Gender", 
       y = "Proportion No-shows")
#No evidence of gender effects



```

#### Neighbourhoods
Some neighbourhoods have high likelihood of misesd appointments, and others lower. However, because there are a limited number observations for some neighbourhoods, the difference in likelihood might be explained by other factors, or simply be person-dependent. This was confirmed by considering the number of people per neighbourhood, sorted by the top and bottom likelihood No-show neighbourhoods. Both high and low likelihood no-show observations tend to have few observations, with minor exceptions. For reference, mean observations per neighourhood is `r df %>% group_by(Neighbourhood) %>% summarize(n = n()) %>% ungroup() %>% summarize(mean = mean(n)) %>% .$mean`  

```{r neighbourhoods, echo = FALSE, fig.align='center', fig.width=5}

#Top 5 no-show areas - low n
df %>% 
  group_by(Neighbourhood) %>%
  summarize(`No_show (High)` = mean(No_show), n = n()) %>%
  arrange(-`No_show (High)`) %>%
  top_n(5, `No_show (High)`) %>%
  knitr::kable(format = "markdown", digits = 3)

```



```{r nhood_2, echo = FALSE, fig.align='center', fig.width=5}

#Bottom 5 no-show areas - also low n
df %>% 
  group_by(Neighbourhood) %>%
  summarize(`No_show (Low)` = mean(No_show), n = n()) %>%
  arrange(`No_show (Low)`) %>%
  top_n(5, -`No_show (Low)`)%>%
  knitr::kable(format = "markdown", digits = 3)


```

The scatterplot confirms that there is additional variation between neighbourhoods, that is not explained by the number of appointments from that neighbourhood. A filter for more than 50 neighbourhoods was applied in the graph to account for an outlier neighbourhood with 100% No-shows.  

```{r neighbourhood_2, echo = FALSE, message=FALSE, fig.align='center', fig.width=5, fig.height=4}

df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  filter(n>50) %>%
    ggplot(aes(n, No_show))+
    geom_point()+
    geom_smooth(method = "auto", se = FALSE)+
    labs(title = "No-show proportion by neighbourhood population",
         subtitle = "Filtered for more than 50 appointments",
       x = "Neighbourhood population", 
       y = "Proportion No-shows")

```

Neighbourhoods were subsequently grouped into quartiles, as shown in the graph. Neighbourhood names were taken out fo ease of presentation. 

```{r neighbourhood_3, echo = FALSE, fig.align='center', fig.width=9, fig.height=4.5}

#Group by neighbourhood group, define variable
Nhood_quartiles<- df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  select(No_show) %>%
  summary() %>%
  as.data.frame() %>%
  slice(c(2, 3, 5)) %>%
  mutate(quartiles = str_sub(Freq, start = -8)) %>% 
  .$quartiles %>%
  as.numeric(digits = 4)

names(Nhood_quartiles)<- c("Below median", "Median", "Above median")

df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  mutate(Nhood_bin = if_else(No_show > median(Nhood_quartiles), 
                             if_else(No_show > max(Nhood_quartiles), "Fourth quartile", "Third quartile"), 
                             if_else(No_show > min(Nhood_quartiles), "Second quartile", "First quartile")),
         Nhood_bin = factor(Nhood_bin, levels = c("First quartile", "Second quartile", "Third quartile", "Fourth quartile"), ordered = TRUE)) %>%
    ggplot(aes(reorder(Neighbourhood, No_show), No_show, fill = Nhood_bin))+
    geom_bar(stat = "identity")+
    scale_fill_manual("Neighbourhood Quartile", values = c("steelblue", "lightblue", "grey50", "grey10"))+
    theme(axis.text.y = element_blank())+
    coord_flip()+
    labs(title = "No-show proportion by neighbourhood",
       x = "Neighbourhood", 
       y = "Proportion No-shows")
  

```



```{r neighbourhood_4, echo = FALSE, include=FALSE}

#Code to store neighbourhood bins in new variable for feature engineering later. 
Nhood_bin<- df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  mutate(Nhood_bin = if_else(No_show > median(Nhood_quartiles), 
                             if_else(No_show > max(Nhood_quartiles), "Fourth_quartile", "Third_quartile"), 
                             if_else(No_show > min(Nhood_quartiles), "Second_quartile", "First_quartile")),
         Nhood_bin = factor(Nhood_bin, levels = c("First_quartile", "Second_quartile", "Third_quartile", "Fourth_quartile"), ordered = TRUE)) %>%
  select(Neighbourhood, Nhood_bin)
 
  
```

### Previously missed appointments
Intuition suggests that someone who have missed a past appointment may be more likely to miss subsequent appointments. This is confirmed by the data, which shows that patients are progressively more likely to miss appointments, the more appointments they have alreay missed. 

There is a limitation in this as all the dataset misses a total patient history. This means that all individuals will start at no past no-shows, and accumulate them over the timeframe of the dataset. 

```{r pastnoshows, echo = FALSE, fig.align='center', message=FALSE}

#Show mean likelihood of missing appointment if missed past appointents
df %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  group_by(PastNo_show) %>%
  summarize(n = n(), No_show = mean(No_show)) %>%
  ggplot(aes(PastNo_show, No_show))+
  geom_point()+
  geom_smooth(method = "auto", se = FALSE)+
  scale_y_continuous(limits = c(0,1.05))+
  labs(title = "No-show proportion by past missed appointments",
     x = "Number of missed appointments", 
     y = "Proportion No-shows")

```



```{r past_no_show_validate, evaluate = FALSE, echo = FALSE, include = FALSE}

#Code to validate that no-shows were estimated correctly. 

#Select highest no-shows to validate number of no-shows
TopNo_shows<- df %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  filter(PastNo_show %in% 14:17) %>% 
  select(PatientId) %>% 
  mutate(PatientId = as.character(PatientId)) %>%
  distinct() %>%
  .$PatientId

#Validating highest number of cumulative No-shows
df %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  mutate(PatientId = as.character(PatientId)) %>%
  filter(PatientId %in% c(TopNo_shows)) %>%
  ggplot(aes(AppointmentDay, PastNo_show, color = PatientId))+
  scale_color_manual("Top-3 No-show Patient IDs", values = c("steelblue", "grey50", "grey10"))+
  geom_line(size = 1)+
   labs(title = "Cumulative No-shows by Appointment Date",
     x = "Appointment Date", 
     y = "Cumulative No-shows")

#appears as though No-shows cumulate correctly based on the input data. 

```

### Medical Information
Medical information includes clinical diagnoses that were captured in the dataset. PreExisting is a variable adding up all conditions, to see if there are additive effects on likelihood of no-shows from having more than one medical condition. Individuals with hypertension were less likely to miss an appointment, as were those with Diabetes, however the latter has a minor effect only. The PreExisting variable does not appear to explain variation beyond what is already captured by the diabetes variable. Furthermore, as different medical conditions appear to influence the likelihood of no-shows in different directions, an additive variable is not likely to add value. Finally, alcoholism and Handicap shows minor to no variations with likelihood of no-shows. 

```{r medical, echo = FALSE, fig.align='center'}

#Make a subset of data with preexisting conditions tidy as one variable. Observe each as an average against dependent variable, as well as the sum. Start with grouping by dependent variable and then taking mean of each and total sum. 
df %>% 
  group_by(No_show) %>%
  summarize(Hypertension = mean(Hypertension),
            Diabetes = mean(Diabetes), 
            Alcoholism = mean(Alcoholism),
            Handicap = mean(Handicap),
            PreExisting = mean(c(Hypertension, Diabetes, Alcoholism, Handicap))) %>%
  pivot_longer(c(Hypertension, Diabetes, Alcoholism, Handicap, PreExisting), names_to = "Condition", values_to = "Proportion", names_repair = "minimal") %>%
    ggplot(aes(reorder(Condition, Proportion), Proportion, fill = No_show))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("No-show", values = c("steelblue", "grey50"))+
    coord_flip()+    
    labs(title = "No-show proportion by Medical Condition",
       x = "Condition", 
       y = "Proportion No-shows")

```

### Other variables
Having received a Scholarship (related to the Bolsa Familia program) shows minor variation only. Having receied an SMS appears to make individuals less likely to make their appointment, which is counterintuitive. This surprising result warrants additional exploration to understand whether the data is reliable, or is inadvertently capturing relationships between other variables and the likelihood of no-shows. 

```{r other, echo = FALSE, fig.align='center', fig.height=3.5, fig.width=4}

df %>% 
  group_by(No_show) %>%
  summarize(Scholarship = mean(Scholarship), 
            SMS_received = mean(SMS_received)) %>%
  pivot_longer(c(Scholarship, SMS_received), names_to = "Protective", values_to = "Proportion", names_repair = "minimal") %>%
    ggplot(aes(reorder(Protective, Proportion), Proportion, fill = No_show))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("No-show", values = c("steelblue", "grey50"))+
    labs(title = "No-show proportion by Other Outcomes",
       x = "Outcome", 
       y = "Proportion No-shows")
  #coord_flip()

```




```{r sms_1, evaluate = FALSE, echo=FALSE, include = FALSE, fig.align='center'}


df %>% 
  group_by (AppointmentDay) %>%
  summarize(SMS_received = mean(SMS_received), 
            BookingDifference = mean(BookingDifference), 
            No_show = mean(No_show)) %>%
  ggplot(aes(AppointmentDay, SMS_received))+
  geom_point(stat = "identity")+
  geom_smooth(aes(x = AppointmentDay, y = No_show), method = "auto", span = 0.4)+
  geom_line(aes(x = AppointmentDay, y = No_show), color = "darkred")+
  scale_color_manual("No-show")

#Check which days there were no SMS's sent out
SMS_down<- df %>% 
  group_by (AppointmentDay) %>%
  summarize(SMS_received = mean(SMS_received), 
            BookingDifference = mean(BookingDifference), 
            No_show = mean(No_show)) %>%
  filter(SMS_received == 0) %>%
  select(AppointmentDay) %>%
  .$AppointmentDay
  
#Downtime in SMSes does not explain variation
df %>% 
  filter(!AppointmentDay %in% SMS_down) %>%
  group_by(No_show) %>%
  summarize(Scholarship = mean(Scholarship), 
            SMS_received = mean(SMS_received)) %>%
  pivot_longer(c(Scholarship, SMS_received), names_to = "Protective", values_to = "Proportion", names_repair = "minimal") %>%
    ggplot(aes(reorder(Protective, Proportion), Proportion, fill = No_show))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("No-show", values = c("steelblue", "grey50"))+
    labs(title = "No-show proportion by Other Outcomes", 
         subtitle = "Filtered out non-SMS days",
       x = "Outcome", 
       y = "Proportion No-shows")


```

It was identified that by analyzing the relationship between SMS received and likelihood of no-shows by appointment type that the correlation between SMS received and No-shows was explained by appointment type, defined by the duration from booking to the appointment. The graph shows that only medium to long-term bookings receive the SMS, and those appointment types (especially longer-term) were already shown to have a higher likelihood of no-showing. In fact, individiuals with long-term bookings are more likely to show up to their appointment if the recieved an SMS reminder.  


```{r sms_2, echo = FALSE}

#Look as SMS reminders for each appointment type - They aren't sent for walk-ins, which already have much lower no-show rates. For apointments within 1 week there is no difference, and for long-term appointments they make patients about 6% more likely to show up to their appointments. 
df %>%
  mutate(SMS_received = ifelse(SMS_received == 0, FALSE, TRUE),
         AppointmentType = ifelse(BookingDifference > 1, ifelse(BookingDifference > 168, "Long-term", "Within one week"), "Walk-in"))%>%
  group_by(AppointmentType, SMS_received) %>%
  summarize(No_show = mean(No_show)) %>%
    ggplot(aes(reorder(AppointmentType, No_show), No_show, fill = SMS_received))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("SMS Received", values = c("steelblue", "grey50"))+
    labs(title = "No-show by appointment type and SMS",
       x = "Appointment Type", 
       y = "Proportion No-shows")


```

## Insights gained
Based on the explorative analysis the following changes were made to the dataset:

Taking out variables reduces noise and increases the likelihood that the machine learning models identify signals present in the dataset. 

**Take out:**

* Unique identifiers, including PatientId, AppointmentId, and Date variables 
* Neighbourhood is replaced by binned variables (quartiles) 
* Handicap, Alcoholism, and gender, as they showed limited variation with No-shows 
* Filter one patient that was recorded as 115 years old (outlier) 
* Drop bins for appointment types and keep continuous scale 

**Add new variables:**

* Number of appointments per day 
* Neighbourhood bins 
* Cumulative missed appointments 


```{r define_df, echo = FALSE}

ml_df <- df %>%
  filter (Age < 100) %>%
  left_join(Nhood_bin, by = "Neighbourhood") %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  group_by(PatientId) %>%
  mutate(AppointmentsPerDay = n() / duration) %>%
  ungroup() %>%
  select(-c(PatientId, AppointmentID, ScheduledDay, AppointmentDay, ScheduledWeekDay, Neighbourhood, Alcoholism, Handicap, Gender))



```


### Feature Engineering
To facilitate algorithm analysis, the following feature engineering was completed:

* Categorical features transformed to factors
  + Appintment week day
  + Neighbourhood bin
  + No-show

* Numerical features centered and scaled
  + Age
  + Scheduled Hour
  + Past No-shows
  + Avg. Appointments Per Day
  + Difference between Booking and Appointment
  
* Binary features left as is
  + Scholarship
  + Hypertension
  + Diabetes
  + SMS_received

A logical test was confirmed that all features had been accounted for. 

```{r preprocessing, echo = FALSE, include = FALSE}

##Preprocessing

#Categorized variables
cat_list <- c("AppointmentWeekDay", "Nhood_bin", "No_show")
ml_df[cat_list] <- lapply(ml_df[cat_list], factor)

#Center and scale numeric columns
#Standardized Variables
stand_list <- c("Age", "ScheduledHour", "PastNo_show", "AppointmentsPerDay", "BookingDifference") 

stand<- preProcess(ml_df[stand_list], method = c("center", "scale"))
print(stand)
summary(ml_df[stand_list])
ml_df[stand_list]<- predict(stand, ml_df[stand_list])


#Variables with no feature engineering
noeng_list <- c("Scholarship", "Hypertension", "Diabetes", "SMS_received")

#confirm that all variables are accounted for. 
length(colnames(ml_df)) == length(c(stand_list, cat_list, noeng_list))

#Confirm that all numeric variables are standardized
lapply(ml_df[stand_list], (hist))

#Confirm that variable names are acceptable
make.names(c(lapply(ml_df[cat_list], levels)), unique = TRUE)


```


## Modeling approach
The goal of the project is to predict whether an appointment will be a No-show or not, which means the problem is well suited to classification models. Roughly 20% of apointments are No-shows, which means that the dataset is imbalanced. All models predict the probability of No-show, which enable optimizing the probability threshold towards higher certainty but fewer correct predictions, against lower certainty but more correct predictions. 

The project followed the following approach to evaluating models:

1. Use a basic classification algorithm with low processing time for preliminary results, and to explore strategies for imbalance mitigation. 
2. Visualize results to better understand which features are most important for predicting No-shows. 
3. Using only the most important features and a reduced dataset to reduce processing time, test a range of more sophisticated models and rank their results. 
4. Run top performing models using the full dataset, and tune parameters for to optimize predictions. 
5. Optimize the probability threshold for the top models. 
6. Make final prediction on the test set using the one best performing model from step 5. 

The data was split into 20 / 80 proportions, where 20% is set aside as the final test set, and 80% is used to train the model(s). Cross-validated versions of the training set is used throughout to evaluate models, until the final prediction on the test set in step 6. 20% was selected as convention, leaving enough data to train in the train set, while keeping enough data in the test set to appropriately evaluate the final trained model. 

### Evaluating models
Accuracy is a common metric to evaluate classification models, measuring the proportion of correct predictions. However, because it is based on sensitivity (true positives) and specificity (true negatives) it is biased for unbalanced datasets. In an extreme situation,the metric creates an incentive for the model to predict all appointments as negatives (i. e. not No-show), with accuracy remaining at 80%.

For this reason the project will use Precision and Recall (Sensitivity) to evaluate models' performance. Precision is used instead of Specificity as it optimizes for the proportion of true positives to all positives (i. e. how many predicted No-shows were actually No-shows). Sensitivity measures what proportion of all No-shows were predicted as No-shows. 

Precision and Sensitivity generally move in the opposite direction. For instance, if Sensitivity is higher and more No-shows were captured overall, Precision is likely to be lower because this usually means there are more false positives. Inversely, when Sensitivity is lower and fewer No-shows were captured overall, Precision is likely to be higher because the model predicts No-shows for more certain observations. The trade-off between Precision and Sensitivity can be optimized using probability threshold optimization. 

For the process of training and comparing different models, ROC will be used as it enables an easier comparison (one number vs. two in the case of Sensitivity and Precision). ROC will be calculated using cross-validation of training data.  

```{r partition, echo = FALSE}

set.seed(1)
#Ceate index to split in test and training sets
test_index <- createDataPartition(y = ml_df$No_show, times = 1, 
                                   p = 0.2, list = FALSE)
train_df <- ml_df[-test_index,] #defining train set
test_df <- ml_df[test_index,] #defining test set

```

  

```{r train_controls, echo = FALSE}

#Set training control method
ctrl<- trainControl(method = "cv", #Set method to 10-fold cv
                    classProbs = TRUE, #Estimate class probabilities when training
                    summaryFunction = twoClassSummary, #Assign type of summary (allows for producing ROC curves)
                    savePredictions = "final") #Allows for using probability threshold tuning

load("ml-2020-06-05.Rdata") #Load past results to make RMD knitting go faster 

```

# Results
This section presents the results from the 6-step approach described previously. 

## 1. Rpart Exploration
The first model uses Rpart as it is a fast-running classification model that can set a baseline to compare other models to. 

```{r model, echo = FALSE, evaluate = FALSE}

# set.seed(1) #Set seed to enable results replication
# 
# model<- "rpart" #Set model as rpart
# ctrl$sampling<- NULL #Don't use sampling strategies yet
# 
# #Train model
# base_fits <- lapply(model, function(model){ 
# 	print(model)
# 	train(No_show ~ .,
#               data = train_df,
#               method = model,
# 	            metric = "ROC",
# 	            trControl = ctrl)
# }) 
<<<<<<< HEAD


```

=======


```

>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb
```{r ROC_1, echo = FALSE, results="hide"}
names(base_fits)<- "rpart - no sampling"

ROC <- sapply(base_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

```

The initial model produced an ROC of 0.578. An ROC of .50 is the equivalent of random guessing, meaning that there is a lot of potential improvement to be made. The maximum ROC value is 1, meaning the model would predict all values perfectly. All reported ROC values represent the average model performance on the cross-validated training set. 

```{r results_1, echo = FALSE}

#Organize and visualize results from initial model
ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

<<<<<<< HEAD
ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  knitr::kable(format = "markdown", digits = 3)
=======
ROC_best_df<- ROC_df %>% 
  arrange(-ROC) %>%
  top_n(5, ROC) #Visualize top 5 models

ROC_best_df %>% knitr::kable(format = "markdown", digits = 3)
>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb

```


```{r sampling, echo = FALSE, evaluate = FALSE}

# set.seed(1)
# 
# #Test different sampling strategies
# model<- c("rpart")
# 
# sampling_method<- c("down", "up", "rose", "smote")
# ctrl$sampling<- sampling_method
# 
# sampling_fits <- lapply(sampling_method, function(sampling_method){ 	print(sampling_method)
# 	train(No_show ~ .,
#               data = train_df,
#               method = model,
# 	            metric = "ROC",
# 	            trControl = ctrl)
# }) 

```



```{r ROC_2, echo = FALSE, results="hide"}

#Create values for table
model<- c("rpart")
sampling_method<- c("down", "up", "rose", "smote")
names(sampling_fits) <- paste(model,"-", sampling_method)

#Create matrix of ROC
sampling_ROC <- sapply(sampling_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

```

Using imbalance strategies help improve results, taking the best model up to roughly 0.7 ROC. Results from different sampling strategies were similar, however smote was selected for being marginally better, as well as having the quality of creating synthetic data similar to the original data, as opposed to reusing or dropping data. 

```{r results_2, echo = FALSE}

#Add results to running top5 
ROC<- append(ROC, sampling_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

<<<<<<< HEAD
ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  top_n(5, ROC) %>%
  knitr::kable(format = "markdown", digits = 3)
=======
ROC_best_df<- ROC_df %>% 
  arrange(-ROC) %>%
  top_n(5, ROC)

ROC_best_df %>% knitr::kable(format = "markdown", digits = 3)
>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb


```

## 2. Visualizing Results, and Variable Importance
Rpart is a tree model, which makes visualizing results easy. The preliminary results indicate that past No-shows are important for predicting future no-shows. Furthermore, age is also important, where high ages are likely to show up, and lower ages less likely to show up. (having seen the data this is an oversimplification, but it holds true directionally). Appointments that were far from their booking date are more likely to be missed. Finally, patients with more frequent appoitments are less likely to miss their appointments. 

```{r tree, echo = FALSE, fig.width=9, fig.height=6}

# Visualize the decision tree with rpart.plot
rpart.plot(sampling_fits$`rpart - smote`$finalModel, box.palette="RdBu", shadow.col="gray", nn=TRUE)

```

Using results from the preliminary analysis we can estimate variable importance for the different features. This enables us the next step of the analysis, which is to evaluate different models using a simpler version of the dataset to reduce processing time. 

The calculated variable importance adds additional nuance to which features are considered important by the model. Importance is estimated on a scale from 0 to 100. In addition to the aforementioned, time of booking, SMS reminder, neighbourhood, and the hypertension variable are also considered marginally important by the model. 

However, looking at the detailed importance values, having booked appointments on Mondays and living in certain neigbourhoods are only ewakly important. an arbitrary cutoff was set at importance of 2, where those below 2 was dropped. Note that unimportant features are only taken out for the model testing phase, and will be put back in for final model training. 

```{r importance, echo = FALSE}

#Calculate and visualize variable importance
imp <- varImp(sampling_fits$`rpart - smote`)

important<- imp$importance %>%
  mutate(Feature = rownames(imp$importance)) %>%
  arrange(-Overall) 

important %>%
  select(Feature, Overall) %>%
  rename(Importance = Overall) %>%
  knitr::kable(format = "markdown", digits = 2)



```

```{r important_fit, echo = FALSE, evaluate = FALSE}

important_vars<- important %>% #Take out feature names to filter dataset 
  filter(Overall > 2) %>%
  select(Feature) %>%
  .$Feature

important_vars <- str_remove(string = important_vars, pattern = "\\.[:alpha:]{1}") #Clean up feature names

important_vars <- append(important_vars, "No_show") #Add dependent variable

#Test rpart results as baseline
# set.seed(1)
# 
# model<- "rpart"
# ctrl$sampling<- NULL
# train_df_imp<- train_df[,important_vars]
# 
# imp_fits <- lapply(model, function(model){ 
# 	print(model)
# 	train(No_show ~ .,
#               data = train_df_imp,
#               method = model,
# 	            metric = "ROC",
# 	            trControl = ctrl)
# }) 

```


```{r ROC_3, echo=FALSE, results="hide"}

names(imp_fits)<- "rpart - no sampling - important vars"

imp_ROC <- sapply(imp_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

```

A sense check confirms that little to no predictive power is lost by taking out the unimportant features. 

```{r results_3, echo = FALSE}

ROC<- append(ROC, imp_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_df %>%
  arrange(-ROC) %>%
<<<<<<< HEAD
  select(model, ROC) %>%
  top_n(6, ROC) %>% 
  knitr::kable(format = "markdown", digits = 3)
=======
  top_n(6, ROC)
>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb


<<<<<<< HEAD
```

=======
>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb

## 3. Testing different models
A number of algorithms were tested on this classification problem. They include but are not limited to boosted trees, k-nearest neighbour, neural net algorithms. To reduce processing time, these algorithms were run on the first 10,000 rows from the training set, and only inlcuding what features were considered to be important, as specified previously. They were also run without smote-sampling for now. 

```{r models, echo = FALSE, evaluate = FALSE}

<<<<<<< HEAD
# set.seed(1)
# 
# #Test different models
# model<- c("naive_bayes","knn", "kknn",
#           "mlp", "monmlp","gbm", "multinom", 
#           "avNNet", "glm", "C5.0")
#  
# sampling_method<- NULL
# ctrl$sampling<- sampling_method
# 
# model_fits <- lapply(model, function(model){ 
# 	print(model)
# 	train(No_show ~ .,
#               data = train_df_imp[1:10000,],
#               method = model,
# 	            metric = "ROC",
# 	            trControl = ctrl)
# }) 

```
=======
## 3. Testing different models
A number of algorithms were tested on this classification problem. They include but are not limited to boosted trees, k-nearest neighbour, neural net algorithms. To reduce processing time, these algorithms were run on the first 10,000 rows from the training set, and only inlcuding what features were considered to be important, as specified previously. They were also run without smote-sampling for now. 
>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb


<<<<<<< HEAD
```{r ROC_4, echo = FALSE, results="hide"}

=======
# set.seed(1)
# 
# #Test different models
# model<- c("naive_bayes","knn", "kknn",
#           "mlp", "monmlp","gbm", "multinom", 
#           "avNNet", "glm", "C5.0")
#  
# sampling_method<- NULL
# ctrl$sampling<- sampling_method
# 
# model_fits <- lapply(model, function(model){ 
# 	print(model)
# 	train(No_show ~ .,
#               data = train_df_imp[1:10000,],
#               method = model,
# 	            metric = "ROC",
# 	            trControl = ctrl)
# }) 

```


```{r ROC_4, echo = FALSE, results="hide"}

>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb
model<- c("naive_bayes","knn", "kknn",
          "mlp", "monmlp","gbm", "multinom", 
          "avNNet", "glm", "C5.0")

names(model_fits) <- paste(model, "- no sampling - important vars")

#Create matrix of accuracy
model_ROC <- sapply(model_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})


```

Using different algorithms improved results beyond what was observed previously, even without using a sampling strategy. Of the 10 tested algorithms, 7 performed better than the smote-sampling rpart model that was trained on the full training set. 


```{r results_4, echo = FALSE}
ROC<- append(ROC, model_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  top_n(8, ROC) %>% 
  knitr::kable(format = "markdown", digits = 3)

```

## 4. Run and tune top performing models on full training set 
The top model was gbm - Generalized Boosted Regression Model. This is a boosted tree algorithm, a class of algorithms which is well suited to classification. The second model is mlp - Multilayer Perceptron. This is a neural net type algorithm. 
<<<<<<< HEAD

While not scoring highly in this instance, the author has had previous success using C5.0 on large datasets. C5.0's success with larger dataset would not be reflected in the 10,000 rows evaluated previously. Therefore, C5.0 will be the third model to be evaluated with smote-sampling and the full dataset.

=======

While not scoring highly in this instance, the author has had previous success using C5.0 on large datasets. C5.0's success with larger dataset would not be reflected in the 10,000 rows evaluated previously. Therefore, C5.0 will be the third model to be evaluated with smote-sampling and the full dataset.

>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb
```{r tune, echo = FALSE, evaluate = FALSE}

# set.seed(1)
# 
# model<- c("gbm", "mlp", "C5.0")
# model
#  
# sampling_method<- "smote"
# ctrl$sampling<- sampling_method
# 
# tune_fits <- lapply(model, function(model){ 
# 	print(model)
# 	train(No_show ~ .,
#               data = train_df,
#               method = model,
# 	            metric = "ROC",
# 	            trControl = ctrl)
# }) 

```

<<<<<<< HEAD


=======


>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb
```{r ROC_5, echo = FALSE, results="hide"}

model<- c("gbm", "mlp", "C5.0")
names(tune_fits) <- paste(model, "- smote - full dataset")

#Create matrix of accuracy
tune_ROC <- sapply(tune_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

```

Results show that adding the full dataset improved results marginally for all three algorithms, with gbm performing slightly better than C5.0, followed by mlp. 

```{r results_5, echo = FALSE}


ROC<- append(ROC, tune_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  top_n(5, ROC) %>% 
  knitr::kable(format = "markdown", digits = 3)
	
```

In considering each model's tuning parameters, C5.0 appears to have a a steeper curve than the other two models. This indicates that the potential for model tuning is higher for C5.0 than for the two other models. 
<<<<<<< HEAD

As a reminder, ROC values reported in the results tables are averages from the models' cross-validated training set. This is why values appear to be higher in the graphs. 

=======

As a reminder, ROC values reported in the results tables are averages from the models' cross-validated training set. This is why values appear to be higher in the graphs. 

>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb
```{r best_fits_results, echo = FALSE}
limits<- c(0.72, 0.78) #Set limits to enable consistent comparisons

ggplot(tune_fits$`C5.0 - smote - full dataset`)+
  scale_y_continuous(limits = limits)+ 
  ggtitle("C5.0, Smote, Full Dataset")

ggplot(tune_fits$`mlp - smote - full dataset`)+
  scale_y_continuous(limits = limits)+
  ggtitle("Mlp, Smote, Full Dataset")

ggplot(tune_fits$`gbm - smote - full dataset`)+
  scale_y_continuous(limits = limits)+
  ggtitle("Gbm, Smote, Full Dataset")

#grid.arrange(c50_params, mlp_params, gbm_params) # Show C5.0 at top with mlp and gb at the bottom

```

For C5.0 winnowing apperas not to make a difference, with tree and rule based models both perform well. However, higher number of boosts seem to improve performance. The final model was trained setting "trials" to 20, 25, and 30. 30 trials was the highest R was able to run without crashing. 

```{r C50_tune, echo = FALSE, evaluate = FALSE}

# set.seed(1)
# 
# getModelInfo("C5.0")
# tune_fits$`C5.0 - smote - full dataset`$bestTune
# 
# #model
# model<- "C5.0"
# model
# 
# #sampling
# sampling_method<- "smote"
# ctrl$sampling<- sampling_method
# 
# #tuning
# grid <- expand.grid(.winnow = c(FALSE), .trials=c(20, 25, 30), .model=c("rules"))
# 
# C50_tune <- lapply(model, function(model){ 
# 	print(model)
# 	train(No_show ~ .,
#               data = train_df,
#               method = model,
# 	            metric = "ROC",
# 	            trControl = ctrl,
# 	            tuneGrid = grid)
# }) 


```



```{r ROC_6, echo = FALSE, results="hide"}

model<- "C5.0"
names(C50_tune) <- paste(model, "- smote - full dataset - tuning")

ggplot(C50_tune$`C5.0 - smote - full dataset - tuning`)+
  ggtitle("C5.0, Smote, Full Dataset, Tuned")


#Create matrix of accuracy
tune_C50 <- sapply(C50_tune, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

```

While the graph indicates additional potential gain, 30 iterations was the highest number that R could run without crashing. With the additional tuning, C5.0 is now the leading model. 

```{r results_6, echo = FALSE}


ROC<- append(ROC, tune_C50)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  top_n(5, ROC) %>% 
  knitr::kable(format = "markdown", digits = 3)


```

## 5. Optimize probability threshold for top models
The probability threshold (i. e. the threshold for when to predict a No-show or not on a scale from 0 to 1 predicted likelihood) can be optimized by using Thresholder. This function uses cross-validated training data to identify the probability threshold which optimize the trade-off between Sensitivity and Precision. 

```{r best, echo = FALSE}

best_models<- list(C50_tune$`C5.0 - smote - full dataset - tuning`,
            tune_fits$`gbm - smote - full dataset`,
            tune_fits$`mlp - smote - full dataset`)

names(best_models)<- c("C5.0", "gbm", "mlp")

  
```

Precision was set to be at least 0.8, with otherwise maximized sensitivity. This was chosen because it is arguably more valuable to a doctor's office to have fewer, but quite certain No-show predictions, than many uncertain ones. 

```{r threshold, echo = FALSE, results="hide", warning=FALSE}

# Calculate the optimum threshold value and set prediction thresholds

precision_threshold<- .8 #This is set to indicate lowest acceptable precision value

prob_thresholds <- sapply(best_models, function(model){ 
print(model)
	#calculate resampling stats
resample_stats <- thresholder(model,threshold = seq(.01, .99, by = 0.04), final = TRUE)
  #Optimize probability threshold
prob_threshold <- resample_stats %>%
  filter(Precision>=precision_threshold) %>%
  filter(Sensitivity==max(Sensitivity)) %>%  
  .$prob_threshold
matrix(prob_threshold)
})

```



```{r optimize_prob, echo = FALSE}

  #Pick out precision and sensitivity from resampled training data
model_stats<- sapply(best_models, function(model){ 
  stats <- thresholder(model,threshold = prob_thresholds[model$method], final = TRUE) %>% 
  select(prob_threshold, Sensitivity, Precision) 
matrix(stats)
})

#Turn stats into data frame
rownames(model_stats)<- c("prob_threshold", "Sensitivity", "Precision")
model_stats<- data.frame(t(model_stats)) %>%
  mutate(model = colnames(model_stats)) %>%
  mutate(Precision = as.numeric(Precision),
         Sensitivity = as.numeric(Sensitivity)) %>%
  rename(Model = model)

#Present data frame in nice table
model_stats %>% 
  select(-prob_threshold) %>%
  select(Model, Precision, Sensitivity) %>%
  knitr::kable(format = "markdown", digits = 3)


```

At high levels of precision, C5.0 had the best trade-off between precision and sensitivity. As a result, the final predictions will be made using the tuned C5.0 model on the testing set. 

## 6. Final predictions on test set using best performing model


```{r metrics, echo = FALSE, warning=FALSE}

best_model<- best_models$C5.0
#Predict values and report on model metrics

#Predict values on testing partition based on optimal probability threshold
PRbest <- predict(best_model, test_df, type = "prob")
pred_best <- ifelse(PRbest$No_show >= prob_thresholds["C5.0"], "No_show", "Present") %>% as.factor()


```

Producing the final model's ROC curve we observe that the final ROC of 0.765 on the test set is consistent although slightly lower than what was achieved using cross-validated training data. 

```{r roc, echo = FALSE, message = FALSE}

#Plot ROC curve 
plot.roc(test_df$No_show, PRbest$No_show, print.auc = TRUE, print.thres = "best", main = "C5.0, Tuned, on Testing set", legacy.axes = TRUE)

```

The final confusion matrix shows that C5.0 correctly predicted roughly 14% of No-shows, at 82% certainty. The higher certainty and lower number of predictions is by design, as mentioned previously.  

```{r confmat_1, echo = FALSE}

#Call confusion matrix
xtab<- confusionMatrix(pred_best, test_df$`No_show`, mode = "prec_recall")
print(xtab)



```

The final figure is a visual representation of the confusion matrix, with the probability threshold indicated by the stapled line. It shows the trade-off between Sensitivity and Precision, in that a lower probability threshold would yield more true positives, but also false positives. 
<<<<<<< HEAD

```{r confmat_2, echo = FALSE}

=======

```{r confmat_2, echo = FALSE}

>>>>>>> 9cf63096f503eb4c82813a2da92b49e6d398ccfb
#Create data frame to visualize confusion matrix
pr_metric_df<- data.frame(obs = test_df$No_show, prob = PRbest$No_show, pred = pred_best)

#Visualize confusion matrix
pr_metric_df %>% 
  mutate(obs=factor(obs, levels=c("No_show","Present"),ordered=TRUE)) %>%
   ggplot(aes(x=obs,y=prob,color=pred))+
   geom_jitter(width = 0.3, height = 0.00, alpha = 0.5)+
     scale_y_continuous(breaks = c(seq.int(0.0, 1, 0.2)))+
     geom_vline(aes(xintercept = 1.5), linetype = "dashed")+
     geom_hline(aes(yintercept = prob_thresholds["C5.0"]), linetype = "dashed")+
     scale_color_manual(values = c("steelblue", "grey45"), name = "Prediction", labels = c("No-Show", "Present"))+
     geom_label(data = data.frame(c(0.65, 0.9)), aes(x = 0.65, y = 0.9), label = str_wrap("True Positives", width = 8), color = "black")+
     geom_label(data = data.frame(c(0.65, 0.1)), aes(x = 0.65, y = 0.1, label = str_wrap("False Negatives", width = 8)), color = "black")+
     geom_label(data = data.frame(c(2.35, 0.1)), aes(x = 2.35, y = 0.1, label = str_wrap("True Negatives", width = 8)), color = "black")+
     geom_label(data = data.frame(c(2.35, 0.9)), aes(x = 2.35, y = 0.9, label = str_wrap("False Positives", width = 8)), color = "black")+
     labs(title = "Confusion Matrix Visual Summary",
          subtitle = "No-show predictions vs. actuals",
          y = "Predicted Probability of No-show",
          x = "Observed Values")


```

# Conclusions and Further Considerations
This report completed a best effort analysis to predcict medical appointment No-shows. It did so testing a range of different models, with emphasis on a basic tree model (Rpart), a multilayer perceptron neural net (mlp), and two boosted tree models; Gradient Boosted Model (gmb) and C5.0. 

The latter three performed best among all the models tested, with C5.0 achieving the best results. The final model predicted 14% of no-shows at 82% precision. A model such as this can improve the efficiency of a doctor's office. 

In the context of this dataset covering about a month, using these predictions could have saved the clinic roughly two thirds of a doctor FTE, assuming 160 hours in a work month. This was estimated by dividing 618 correct predictions by 6 (assuming 6 appointments per hour), resulting in 103 hours saved.  

## Ethical and practical considerations
Care should be taken when using a model such as this to predict No-shows, as it could result in unintended consequences such as discrimination against groups whose circumstances make them more likely to miss their appointments more often. No patients should face situations where their appointment was cancelled because a model expected the patient not to show up. 

Consequently, the way results are used should be developed with care. One suggestion could be to adjust the expected length of appointments in the booking system by their likelihood of being missed, and fill up the total booking time of the course of the day. This would take advantage of the overall predictions for the whole day, instead of cancelling appointments for individuals. Additional work should be done around designing policies for applying machine learning models to improve doctor's offices' efficiency. 

## Features and further exploration
The model indicated that patient age, past no-show history, time from booking to appointment, and booking frequency are important features for no-show prediction. 

The presence of negative values for some variables (i. e. time from booking to appointment) meant that they could not undergo log transformation, despite demonstrating a distribution that would lend itself well to said transformation. Access to data without such negative values could potentially improve results.

Another iteration could split data according to time, so that the model is trained on appointments happening at the beginning of the timeframe, and the testing is done on appointments at the end of the timeframe. This would avoid the issue of having all patients beginning at 0 past no-shows. However, new patients will always have 0 no-shows. The implication from this is that predictions on new patients without any history is less accurate than predictions for individuals with many past appointments. 

Finally, the data reflected patients and appointments in Brazil. Results may not be immediately transferable to other contexts. 

