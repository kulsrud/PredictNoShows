---
title: "Predicting Medical Appointment No-shows"
author: "Knut Ulsrud"
date: "6/4/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load, echo=FALSE, include = FALSE}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org") #For general data cleaning
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org") #For creating test and train set
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org") #For displaying graphs side by side
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org") #For manipulating date variables
if(!require(woeBinning)) install.packages("WoeBinning", repos = "http://cran.us.r-project.org") #For binning variables automatically

#ML Packages
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org") #For running some caret functions
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org") #For producing ROC curves
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org") #For visualizing rpart plots
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org") #For rpart functions
if(!require(DMwR)) install.packages("DMwR", repos = "http://cran.us.r-project.org") #For running ml


url<- "https://github.com/kulsrud/PredictNoShows/raw/master/" 
file<- "KaggleV2-May-2016.csv"
df<- read.csv(paste(url, file, sep = ""))

#load("ml-2020-04-27.Rdata")

```

# Overview
This section presents the project's goals and a high-level overview of project data. 

## Goal of the project and key steps that were performed
This project's goal was to predict attendance (i. e. No-show vs. attended) for doctor's appointments based on publicly available medical appointment data. This report was produced as a submission to the Data Science: Capstone course provided by HarvardX on the EdX platform.

## Dataset Description
The medical appointment dataset  has 110,527 rows and 14 columns, and contains patient attendance data from April 29 to June 8, 2016, for a clinic in Brazil. The original dataset was sourced via Kaggle (ADD HYPERLINK). Columns contain the following information:  

Variable name |Variable Description
--------------|------------------------------------------------------------------------
1. PatientId | Unique identifier for each patient
2. AppointmentID | Unique identifier for each appointment
3. Gender | Male or Female
4. ScheduledDay | When the appointment was scheduled
5. AppointmentDay | When the appointment happened
6. Age | Patient age
7. Neighbourhood | What neighbourhood patient lived in
8. Scholarship | Binary variable related to scholarship reception through the Bolsa Familia program
9. Hypertension | Whether the patient lived with hypertension
10. Diabetes | Whether the patient lived with diabetes
11. Alcoholism | Whether the patient lived with alcoholism
12. Handicap | Whether the patient lived with a physical handicap (i. e. blindness, inability to walk, etc.)
13. SMS_received | Whether the patient received an SMS reminder
14. No_show | Whether the patient showed up for their appointment



The following table shows the first four rows of the dataset.  

```{r description, echo=FALSE}

#Correct typos in variable names
names(df)[names(df)=="Hipertension"]<- "Hypertension"
names(df)[names(df)=="Handcap"]<- "Handicap"
names(df)[names(df)=="No.show"]<- "No_show"

#To show top 4 rows of sample data
knitr::kable(head(df, 4), "markdown") 

```

# Methods and Analysis
This section presents steps taken to perform initial analysis on the dataset. It includes the following sections: Data Preaparation, Data Exploration and Visualization, Insights Gained, and Modelling Approach.  

## Data Preparation
A high-level check revealed that there were no missing values in the appointment dataset. The data was already presented in a tidy format (one row equals one observation / appointment) which means no additional transformation was required to start using the dataset.

There are two date columns which were provided in year-month-date -- hour-minute-second format (i. e. "2016-04-29T00:00:00Z"). To facilitate analysis, both ScheduledDay and AppointmentDay columns were transformed into the following:

* Day of week
* Hour of day (only available for ScheduledDay)
* Duration from booking to appointment

For AppointmentDay all hour-minute-second data was set to 00:00:00. This means that calculated duration from booking to appointment might err by the number of hours from 00:00:00 to clinic closing, at which time the last appointment would have taken place. 

```{r cleaning, echo = FALSE, include = FALSE}

head(df) #The dataset is tidy, with each row representing one appointment slot 
colSums((is.na(df))) #There are no missing values in the dataset

#Transformed date variables to date format. 
df<- df %>% mutate(ScheduledDay = as_datetime(str_remove(str_replace(ScheduledDay, "T", " "), "Z")),
                      AppointmentDay = as_datetime(str_remove(str_replace(AppointmentDay, "T", " "), "Z")),
                      AppointmentWeekDay = weekdays(AppointmentDay),
                      ScheduledWeekDay = weekdays(ScheduledDay),
                      ScheduledHour = hour(ScheduledDay),
                      No_show = ifelse(No_show=="No", FALSE, TRUE),
                      BookingDifference = as.numeric(AppointmentDay - ScheduledDay))


```

## Data Exploration and Visualization
It is likely that there is systematic variation between patient specific information and whether or not they No-show. This section will explore this variation according to 

* Scheduling and appointment timing
* Past no-shows
* Demographics effects (i. e. age, gender, etc.)
* Medical information
* Other binary identifiers

It will also explore basic characteristics of variables of interest.

As shown in the table, roughly 20% of appointments in the dataset were missed.

```{r base, echo = FALSE}

#Estimate overall No-show proportion
knitr::kable(prop.table(table(df$No_show)), col.names = c("No-show", "Proportion"), format = "markdown", digits = 3)

#Highlight total duration of dataset
min_date<- min(df$AppointmentDay)
max_date<- max(df$AppointmentDay)
myDates <-seq(from = min_date, to = max_date, by = "days")
duration<- length(which(wday(myDates) %in% c(2:6)))
#Number of weekdays covered: 


```

 `r print(paste("The dataset covers", duration, "days, from", min_date, "to", max_date))`

### Appointment Timing
This section explores variation between time-related variables and appointment No-shows. 

#### Appointment and scheduling Weekday
At first glance it appears that appointments that were scheduled on Saturdays were much less likely to be missed. Otherwise, scheduling day does not appear to vary by meaningful proportions. 

```{r weekday, echo = FALSE}

#Look at No-shows by weekday
scheduled<- df %>%
     group_by(ScheduledWeekDay) %>% #To see individual movies by name
     summarize(No_show = mean(No_show)) %>% 
     ggplot(aes(factor(ScheduledWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"), ordered = TRUE), No_show))+
     geom_bar(stat = "identity")+
     coord_flip()+
    scale_y_continuous(limits = c(0, 0.25))+
     labs(title = "No-show proportion, by weekday scheduled",
       x = NULL, 
       y = NULL)

appointment<- df %>%
    group_by(AppointmentWeekDay) %>% #To see individual movies by name
     summarize(No_show = mean(No_show)) %>% 
      ggplot(aes(factor(AppointmentWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"), ordered = TRUE), No_show))+
      geom_bar(stat = "identity")+
      coord_flip()+
      scale_y_continuous(limits = c(0, 0.25))+
      labs(title = "No-show proportion, by appointment weekday",
       x = NULL, 
       y = NULL)

grid.arrange(scheduled, appointment)

```

On the other hand, appointents held on Saturdays appear to be missed more often than other days, while Thursday appointments are kept most often. There is minor variation between other days of the week.

Further exploration reveal that that much fewer appointments were scheduled and held on Saturdays than on other days. This indicates that Saturday data may be less reliable than other weekdays, as it may be susceptible to for instance selection bias. 

```{r saturdays, echo = FALSE}

#Scheduled on Saturdays vs. No-show
df %>% 
  group_by(factor(ScheduledWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  summarize(n = n(), No_show = mean(No_show)) %>%
  knitr::kable(digits = 2, col.names = c("Scheduled Weekday", "Appointments", "Percent No-show"))
```

 

```{r saturdays_3, echo = FALSE}

#Appointment on Saturdays vs. Walk-ins
df %>% 
  group_by(factor(AppointmentWeekDay, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  summarize(n = n(), No_show = mean(No_show)) %>%
  knitr::kable(digits = 2, col.names = c("Appointment Weekday", "Appointments", "Percent No-show"))

```

Instead of filtering for Saturday appointments and removing useful data, a more prudent decision would be to drop Weekday Scheduled. 

Saturday appointment effects are small enough that it should not meaningfully skew the data, and the variable's importance to the final model could be tested. This would confirm whether the variation between appointment weekdays is explained by other variables. 

#### Hour of day scheduled
The hour of day that appointments were scheduled also seems to have an impact, varying from around 15% No-shows early in the morning, to over 30% late in the evening. It is unclear why this should be the case, other than that patients may be more tired and forgetful and therefore forget appointments scheduled later in the day. 

```{r hours, echo = FALSE}

#Look at No-shows by hour of the day
df %>%
     group_by(ScheduledHour) %>% #To see individual movies by name
     summarize(No_show = mean(No_show)) %>% 
     ggplot(aes(ScheduledHour, No_show))+
        geom_bar(stat = "identity")+
  labs(title = "Appointment no-show proportion",
       x = "Hour of day scheduled", 
       y = "No-show proportion")

```

#### Difference between scheduling and appointment
By binning the difference between scheduled time and appointment time we see that there is substantial and systematic variation between time differences, and No-shows. 

Interestingly there are appointments that are recorded as scheduled after the appointment took place. This has two potential explanations. One is the aforementioned issue where exact appointment time was not reported, only date and 00:00:00 for hour-minute-second. This would mean that any same-day scheduled appointment would appear negative. Second, a patient walk-in could happen, and entered into the system by an administrator later. 

Either way, the recorded difference between scheduling and appointment timing deviates from the true difference by a number of hours. This is made clear by the presence of negative values. This paper continues the timing difference exploration with this caveat in mind. 

```{r wait, echo = FALSE}
#Bin difference between scheduled time and appointment time
df %>%
  group_by(BookDiffBin = ntile(BookingDifference, 12)) %>%
  summarize(No_show = mean(No_show), BookDiff = mean(BookingDifference)) %>%
  ggplot(aes(BookDiffBin, No_show))+
  geom_bar(stat = "identity", size = 1)+
  #geom_step(aes(ntile(BookDiffBin, 3)))
  geom_label(aes(x = BookDiffBin, y = No_show + 0.05, label = round(BookDiff, 0)))+
  scale_x_continuous(breaks = c(seq.int(1, 12, 1)))+
  labs(title = "Avg difference scheduling and appointment",
       x = "Bin no.", 
       y = "No-show proportion")

```

To do a high-level exploration of No-show outcomes it is reasonable to bin the hours difference data. This is especially the case given that individual observations might vary within a few hours from their actual time, as just explained. This would smooth data recording errors while retaining high-level variation. 

Binning was based on the intuition that three common appointment types vary by Walk-ins (less than 1 hours difference), Scheduled within a week (1 to 168 hours difference), and Scheduled within more than one week (more than 168 hours difference)


```{r bins, echo = FALSE}

# ntile_bins<- df %>%
#   group_by(BookDiffBin = ntile(BookingDifference, 3)) %>%
#   summarize(No_show = mean(No_show), BookDiff = mean(BookingDifference)) %>%
#   ggplot(aes(BookDiffBin, No_show))+
#   geom_bar(stat = "identity", size = 1)+
#   #geom_step(aes(ntile(BookDiffBin, 3)))
#   geom_label(aes(x = BookDiffBin, y = No_show + 0.05,    label = round(BookDiff, 0)))+
#   scale_x_discrete(breaks = NULL)+
#   scale_y_continuous(limits = c(0, 0.4))+
#   #theme(axis.text.x = element_text(angle = 45, hjust = 1))+
#   labs(title = "Binned by frequency",
#        x = NULL, 
#        y = "No-show proportion")

#Intuition about walk-ins
df %>%
  mutate(AppointmentType = ifelse(BookingDifference > 1, ifelse(BookingDifference > 168, "Long-term", "Within one week"), "Walk-in")) %>%
  group_by(AppointmentType) %>%
  summarize(No_show = mean(No_show), BookDiff = mean(BookingDifference)) %>%
  ggplot(aes(reorder(AppointmentType,No_show), No_show))+
  geom_bar(stat = "identity", size = 1)+
  geom_label(aes(x = AppointmentType, y = No_show + 0.05, label = round(BookDiff, 0)))+
  #scale_x_discrete(breaks = NULL)+
  scale_y_continuous(limits = c(0, 0.4))+
  #theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(title = "No-show likelihood, Binned by Appointment Type",
       x = "Appointment Type", 
       y = "Likelihood of No-show")

# #Binning - less applicable than insight about walk-ins
# bins<- woe.binning(df = df, target.var = 'No_show', pred.var = 'BookingDifference')
# auto_bin<- (woe.binning.deploy(df, bins) %>%
#               pull(as.numeric(BookingDifference.binned)))
# 
# auto<- df %>%
#   mutate(auto_bin = auto_bin) %>% 
#   group_by(auto_bin) %>%
#   summarize(No_show = mean(No_show), BookDiff = mean(BookingDifference)) %>%
#   ggplot(aes(auto_bin, No_show))+
#   geom_bar(stat = "identity", size = 1)+
#   geom_label(aes(x = auto_bin, y = No_show + 0.05, label = round(BookDiff, 0)))+
#   scale_x_discrete(breaks = NULL)+
#   scale_y_continuous(limits = c(0, 0.4))+
#   #theme(axis.text.x = element_text(angle = 45, hjust = 1))+
#   labs(title = "Binned by woeBinning",
#        x = NULL, 
#        y = NULL)
# 
# grid.arrange(ntile_bins, intuitive, auto, ncol = 3,
#              widths = c(2.5, 2, 2))

```

Binning shows that the intuition about appointment types is reasonable. This intuition can be used further in exploratory analysis. 

### Demographics 
This section explores variations in the No-show outcome by different demographic factors: 

* Patient effects 
* Age 
* Gender 
* City Neighbourhood patient lives in 

#### Patient effects
The scatterplot confirms the intution that there are differences in how many appointments patients had over the period of the dataset, and how likely they were to miss their apointments. For instance, there is a sharp cutoff in No-show likelihood around an average of one appointment per day. In other words, patients who see their doctor more frequently are more likely to keep their appointments. The observation that some patients have more than one appointment per day is reasonable. This is because one longer appointment would likely be scheduled as a sequence of individual appointments in medical scheduling software, giving the appearance of multiple appointments. 

```{r demographics, echo = FALSE}

#Looking at patient effects
df %>%
  group_by(PatientId) %>%
  summarize(No_show = mean(No_show), 
            n = n(), 
            AppointmentsPerDay = n / duration) %>% #Assuming that multiple appointments per day means longer appointments (i. e. units scheduled back to back)
  ggplot(aes(AppointmentsPerDay, No_show))+
  geom_point()+
  labs(title = "No-show proportion by patient ID",
       x = "Average number of appointments per day", 
       y = "Average proportion No-shows")
  
```

#### Age
There is a clear visual relationship between age and likelihood of no-shows. Patients in their 20s seem to miss appointments more often than other age groups, followed by gradually lower chance of No-show as patients age. Older patients diverge from this pattern, where some patients are reliable, and others not. 

```{r age, echo = FALSE, warning=FALSE}

#Looking at age effects
df %>% 
  filter(Age < 100) %>% 
  group_by(Age) %>%
  summarize(No_show = mean(No_show), n = n()) %>% 
  ggplot(aes(Age, No_show))+
  geom_point()+
  geom_smooth(method = "auto")+
  labs(title = "No-show proportion by age",
       x = "Age", 
       y = "Average proportion No-shows")

```

#### Gender
There does not appear to be any obvious connections between gender and likelihood of No-shows. 

```{r gender, echo = FALSE}

#Looking at Gender effects
df %>%
  group_by(Gender) %>%
  summarize(No_show = mean(No_show)) %>%
  ggplot(aes(Gender, No_show))+
  geom_bar(stat = "identity")+
  labs(title = "No-show proportion by gender",
       x = "Gender", 
       y = "Proportion No-shows")
#No evidence of gender effects



```

#### Neighbourhoods
Some neighbourhoods have high likelihood of misesd appointments, and others lower. However, because there are a limited number observations for some neighbourhoods, the difference in likelihood might be explained by other factors, or simply be person-dependent. This was confirmed by considering the number of people per neighbourhood, sorted by the top and bottom likelihood No-show neighbourhoods. Both the top and bottom neighbourhoods have few observations. 

```{r neighbourhoods, echo = FALSE}

#Top 5 no-show areas - low n
df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  arrange(-No_show) %>%
  top_n(5, No_show) %>%
  knitr::kable(format = "markdown", digits = 3)

```

```{r nhood_2}

#Bottom 5 no-show areas - also low n
df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  arrange(No_show) %>%
  top_n(5, -No_show)%>%
  knitr::kable(format = "markdown", digits = 3)


```

The scatterplot confirms that there is additional variation between neighbourhoods, that is not explained by the number of appointments from that neighbourhood. A filter for more than 50 neighbourhoods was applied in the graph to account for an outlier neighbourhood with 100% No-shows.  

```{r neighbourhood_2, echo = FALSE, warning=FALSE}

df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  filter(n>50) %>%
    ggplot(aes(n, No_show))+
    geom_point()+
    geom_smooth(method = "auto", se = FALSE)+
    labs(title = "No-show proportion by neighbourhood population",
         subtitle = "Filtered for more than 50 appointments",
       x = "Neighbourhood population", 
       y = "Proportion No-shows")

```

Neighbourhoods were subsequently grouped into quartiles, as shown in the graph. Neighbourhood names were taken out fo ease of presentation. 

```{r neighbourhood_3, echo = FALSE}

#Group by neighbourhood group, define variable
Nhood_quartiles<- df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  select(No_show) %>%
  summary() %>%
  as.data.frame() %>%
  slice(c(2, 3, 5)) %>%
  mutate(quartiles = str_sub(Freq, start = -8)) %>% 
  .$quartiles %>%
  as.numeric(digits = 4)

names(Nhood_quartiles)<- c("Below median", "Median", "Above median")

df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  mutate(Nhood_bin = if_else(No_show > median(Nhood_quartiles), 
                             if_else(No_show > max(Nhood_quartiles), "Fourth quartile", "Third quartile"), 
                             if_else(No_show > min(Nhood_quartiles), "Second quartile", "First quartile")),
         Nhood_bin = factor(Nhood_bin, levels = c("First quartile", "Second quartile", "Third quartile", "Fourth quartile"), ordered = TRUE)) %>%
    ggplot(aes(reorder(Neighbourhood, No_show), No_show, fill = Nhood_bin))+
    geom_bar(stat = "identity")+
    scale_fill_manual("Neighbourhood Quartile", values = c("steelblue", "lightblue", "grey50", "grey10"))+
    theme(axis.text.y = element_blank())+
    coord_flip()+
    labs(title = "No-show proportion by neighbourhood",
       x = "Neighbourhood", 
       y = "Proportion No-shows")
  

```



```{r neighbourhood_4, echo = FALSE, include=FALSE}

#Code to store neighbourhood bins in new variable for feature engineering later. 
Nhood_bin<- df %>% 
  group_by(Neighbourhood) %>%
  summarize(No_show = mean(No_show), n = n()) %>%
  mutate(Nhood_bin = if_else(No_show > median(Nhood_quartiles), 
                             if_else(No_show > max(Nhood_quartiles), "Fourth_quartile", "Third_quartile"), 
                             if_else(No_show > min(Nhood_quartiles), "Second_quartile", "First_quartile")),
         Nhood_bin = factor(Nhood_bin, levels = c("First_quartile", "Second_quartile", "Third_quartile", "Fourth_quartile"), ordered = TRUE)) %>%
  select(Neighbourhood, Nhood_bin)
 
  
```

### Previously missed appointments
Intuition suggests that someone who have missed a past appointment may be more likely to miss subsequent appointments. This is confirmed by the data, which shows that patients are progressively more likely to miss appointments, the more appointments they have alreay missed. 

There is a limitation in this as all the dataset misses a total patient history. This means that all individuals will start at no past no-shows, and accumulate them over the timeframe of the dataset. 

```{r pastnoshows, echo = FALSE}
#No show if patient missed a past appointment
# df %>%
#   arrange(AppointmentDay) %>%
#   group_by(PatientId) %>%
#   mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
#   ungroup() %>%
#   arrange(PatientId, -PastNo_show) %>%
#   group_by(PastNo_show) %>%
#   summarize(n = n(), No_show = mean(No_show)) %>%
#   knitr::kable()
  
#Show mean likelihood of missing appointment if missed past appointents
df %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  group_by(PastNo_show) %>%
  summarize(n = n(), No_show = mean(No_show)) %>%
  ggplot(aes(PastNo_show, No_show))+
  geom_point()+
  geom_smooth(method = "auto", se = FALSE)+
  scale_y_continuous(limits = c(0,1.05))+
  labs(title = "No-show proportion by past missed appointments",
     x = "Number of missed appointments", 
     y = "Proportion No-shows")

```



```{past_no_show_validate, evaluate = FALSE, echo = FALSE, include = FALSE}

#Code to validate that no-shows were estimated correctly. 

#Select highest no-shows to validate number of no-shows
TopNo_shows<- df %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  filter(PastNo_show %in% 14:17) %>% 
  select(PatientId) %>% 
  mutate(PatientId = as.character(PatientId)) %>%
  distinct() %>%
  .$PatientId

#Validating highest number of cumulative No-shows
df %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  mutate(PatientId = as.character(PatientId)) %>%
  filter(PatientId %in% c(TopNo_shows)) %>%
  ggplot(aes(AppointmentDay, PastNo_show, color = PatientId))+
  scale_color_manual("Top-3 No-show Patient IDs", values = c("steelblue", "grey50", "grey10"))+
  geom_line(size = 1)+
   labs(title = "Cumulative No-shows by Appointment Date",
     x = "Appointment Date", 
     y = "Cumulative No-shows")

#appears as though No-shows cumulate correctly based on the input data. 

```

### Medical Information
Medical information includes clinical diagnoses that were captured in the dataset. PreExisting is a variable adding up all conditions, to see if there are additive effects on likelihood of no-shows from having more than one medical condition. Individuals with hypertension were less likely to miss an appointment, as were those with Diabetes, however the latter has a minor effect only. The PreExisting variable does not appear to explain variation beyond what is already captured by the diabetes variable. Furthermore, as different medical conditions appear to influence the likelihood of no-shows in different directions, an additive variable is not likely to add value. Finally, alcoholism and Handicap shows minor to no variations with likelihood of no-shows. 

```{r medical, echo = FALSE}

#Make a subset of data with preexisting conditions tidy as one variable. Observe each as an average against dependent variable, as well as the sum. Start with grouping by dependent variable and then taking mean of each and total sum. 
df %>% 
  group_by(No_show) %>%
  summarize(Hypertension = mean(Hypertension),
            Diabetes = mean(Diabetes), 
            Alcoholism = mean(Alcoholism),
            Handicap = mean(Handicap),
            PreExisting = mean(c(Hypertension, Diabetes, Alcoholism, Handicap))) %>%
  pivot_longer(c(Hypertension, Diabetes, Alcoholism, Handicap, PreExisting), names_to = "Condition", values_to = "Proportion", names_repair = "minimal") %>%
    ggplot(aes(reorder(Condition, Proportion), Proportion, fill = No_show))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("No-show", values = c("steelblue", "grey50"))+
    coord_flip()+    
    labs(title = "No-show proportion by Medical Condition",
       x = "Condition", 
       y = "Proportion No-shows")

```

### Other variables
Having received a Scholarship (related to the Bolsa Familia program) shows minor variation only. Having receied an SMS appears to make individuals less likely to make their appointment, which is counterintuitive. This surprising result warrants additional exploration to understand whether the data is reliable, or is inadvertently capturing relationships between other variables and the likelihood of no-shows. 

```{r other, echo = FALSE}

df %>% 
  group_by(No_show) %>%
  summarize(Scholarship = mean(Scholarship), 
            SMS_received = mean(SMS_received)) %>%
  pivot_longer(c(Scholarship, SMS_received), names_to = "Protective", values_to = "Proportion", names_repair = "minimal") %>%
    ggplot(aes(reorder(Protective, Proportion), Proportion, fill = No_show))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("No-show", values = c("steelblue", "grey50"))+
    labs(title = "No-show proportion by Other Outcomes",
       x = "Outcome", 
       y = "Proportion No-shows")
  #coord_flip()

```




```{r sms_1, evaluate = FALSE, echo=FALSE, include = FALSE}


df %>% 
  group_by (AppointmentDay) %>%
  summarize(SMS_received = mean(SMS_received), 
            BookingDifference = mean(BookingDifference), 
            No_show = mean(No_show)) %>%
  ggplot(aes(AppointmentDay, SMS_received))+
  geom_point(stat = "identity")+
  geom_smooth(aes(x = AppointmentDay, y = No_show), method = "auto", span = 0.4)+
  geom_line(aes(x = AppointmentDay, y = No_show), color = "darkred")+
  scale_color_manual("No-show")

#Check which days there were no SMS's sent out
SMS_down<- df %>% 
  group_by (AppointmentDay) %>%
  summarize(SMS_received = mean(SMS_received), 
            BookingDifference = mean(BookingDifference), 
            No_show = mean(No_show)) %>%
  filter(SMS_received == 0) %>%
  select(AppointmentDay) %>%
  .$AppointmentDay
  
#Downtime in SMSes does not explain variation
df %>% 
  filter(!AppointmentDay %in% SMS_down) %>%
  group_by(No_show) %>%
  summarize(Scholarship = mean(Scholarship), 
            SMS_received = mean(SMS_received)) %>%
  pivot_longer(c(Scholarship, SMS_received), names_to = "Protective", values_to = "Proportion", names_repair = "minimal") %>%
    ggplot(aes(reorder(Protective, Proportion), Proportion, fill = No_show))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("No-show", values = c("steelblue", "grey50"))+
    labs(title = "No-show proportion by Other Outcomes", 
         subtitle = "Filtered out non-SMS days",
       x = "Outcome", 
       y = "Proportion No-shows")


```

It was identified that by analyzing the relationship between SMS received and likelihood of no-shows by appointment type that the correlation between SMS received and No-shows was explained by appointment type, defined by the duration from booking to the appointment. The graph shows that only medium to long-term bookings receive the SMS, and those appointment types (especially longer-term) were already shown to have a higher likelihood of no-showing. In fact, individiuals with long-term bookings are more likely to show up to their appointment if the recieved an SMS reminder.  


```{r sms_2, echo = FALse}

#Look as SMS reminders for each appointment type - They aren't sent for walk-ins, which already have much lower no-show rates. For apointments within 1 week there is no difference, and for long-term appointments they make patients about 6% more likely to show up to their appointments. 
df %>%
  mutate(SMS_received = ifelse(SMS_received == 0, FALSE, TRUE),
         AppointmentType = ifelse(BookingDifference > 1, ifelse(BookingDifference > 168, "Long-term", "Within one week"), "Walk-in"))%>%
  group_by(AppointmentType, SMS_received) %>%
  summarize(No_show = mean(No_show)) %>%
    ggplot(aes(reorder(AppointmentType, No_show), No_show, fill = SMS_received))+
    geom_bar(stat = "identity", position = "dodge")+
    scale_fill_manual("SMS Received", values = c("steelblue", "grey50"))+
    labs(title = "No-show proportion by appointment type and SMS receipt",
       x = "Appointment Type", 
       y = "Proportion No-shows")


```

## Insights gained
Based on the explorative analysis the following changes were made to the dataset:

Taking out variables reduces noise and increases the likelihood that the machine learning models identify signals present in the dataset. 

**Take out:**

* Unique identifiers, including PatientId, AppointmentId, and Date variables 
* Neighbourhood is replaced by binned variables (quartiles) 
* Handicap, Alcoholism, and gender, as they showed limited variation with No-shows 
* Filter one patient that was recorded as 115 years old (outlier) 
* Drop bins for appointment types and keep continuous scale 

**Add new variables:**

* Number of appointments per day 
* Neighbourhood bins 
* Cumulative missed appointments 


```{r define_df, echo = FALSE}

ml_df <- df %>%
  filter (Age < 100) %>%
  left_join(Nhood_bin, by = "Neighbourhood") %>%
  arrange(AppointmentDay) %>%
  group_by(PatientId) %>%
  mutate(PastNo_show = ifelse(cumsum(No_show)-1 <0, 0, cumsum(No_show)-1)) %>%
  ungroup() %>%
  group_by(PatientId) %>%
  mutate(AppointmentsPerDay = n() / duration) %>%
  ungroup() %>%
  select(-c(PatientId, AppointmentID, ScheduledDay, AppointmentDay, ScheduledWeekDay, Neighbourhood, Alcoholism, Handicap, Gender))



```


### Feature Engineering
To facilitate algorithm analysis, the following feature engineering was completed:

* Categorical features transformed to factors
  + Appintment week day
  + Neighbourhood bin
  + No-show

* Numerical features centered and scaled
  + Age
  + Scheduled Hour
  + Past No-shows
  + Avg. Appointments Per Day
  + Difference between Booking and Appointment
  
* Binary features left as is
  + Scholarship
  + Hypertension
  + Diabetes
  + SMS_received

A logical test was conducted to confirm that all features had been accounted for. 

```{r preprocessing, echo = FALSE, include = FALSE}

##Preprocessing

#Categorized variables
cat_list <- c("AppointmentWeekDay", "Nhood_bin", "No_show")
ml_df[cat_list] <- lapply(ml_df[cat_list], factor)

#Center and scale numeric columns
#Standardized Variables
stand_list <- c("Age", "ScheduledHour", "PastNo_show", "AppointmentsPerDay", "BookingDifference") 

stand<- preProcess(ml_df[stand_list], method = c("center", "scale"))
print(stand)
summary(ml_df[stand_list])
ml_df[stand_list]<- predict(stand, ml_df[stand_list])


#Variables with no feature engineering
noeng_list <- c("Scholarship", "Hypertension", "Diabetes", "SMS_received")

#confirm that all variables are accounted for. 
length(colnames(ml_df)) == length(c(stand_list, cat_list, noeng_list))

#Confirm that all numeric variables are standardized
lapply(ml_df[stand_list], (hist))

#Confirm that variable names are acceptable
make.names(c(lapply(ml_df[cat_list], levels)), unique = TRUE)

knitr::knit_exit()

```
  


## Modeling approach
The model will follow the following formula to predict movie scores.  

$Y_{u,i} = μ + b_i +b_u + ∑k = 1^{K}x_{u,i}β_{k} + ε_{u,i}$, with  $x^{k}_{u,i}=1$ if $g_{u,i}$ is genre $k$  

It will calculate movie effects $b_i$, user effects $b_u$, and genre effects $k$ that summarize genre scores for all genres that apply to a particular movie rating by a user.  

### Evaluating models
Accuracy
Precision and Recall (Sensitivity)


```{r partition, echo = FALSE, evaluate = FALSE}

set.seed(1)
#Ceate index to split in test and training sets
test_index <- createDataPartition(y = ml_df$No_show, times = 1, 
                                   p = 0.2, list = FALSE)
train_df <- ml_df[-test_index,] #defining train set
test_df <- ml_df[test_index,] #defining test set

```

# Results
A movie recommendation system based on the presented approach produces the following RMSE:  

```{r train_controls, echo = FALSE, evaluate = FALSE}

#Set training control method for both Conservative and Non-Conservative models
ctrl<- trainControl(method = "cv", #Set method to 10-fold cv
                    classProbs = TRUE, #Estimate class probabilities when training
                    summaryFunction = twoClassSummary, #Assign type of summary (allows for producing ROC curves)
                    savePredictions = "final") #Allows for using probability threshold tuning

```


```{r model, echo = FALSE, evaluate = FALSE}

model<- "rpart"
ctrl$sampling<- NULL
base_fits <- lapply(model, function(model){ 
	print(model)
	train(No_show ~ .,
              data = train_df,
              method = model,
	            metric = "ROC",
	            trControl = ctrl)
}) 

names(base_fits)<- "rpart - no sampling"

ROC <- sapply(base_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})
ROC


```


```{r sampling, echo = FALSE, evaluate = FALSE}

#Test different sampling strategies
model<- c("rpart")

sampling_method<- c("down", "up", "rose", "smote")
ctrl$sampling<- sampling_method

sampling_fits <- lapply(sampling_method, function(sampling_method){ 	print(sampling_method)
	train(No_show ~ .,
              data = train_df,
              method = model,
	            metric = "ROC",
	            trControl = ctrl)
}) 
    
names(sampling_fits) <- paste(model,"-", sampling_method)

#Create matrix of accuracy
sampling_ROC <- sapply(sampling_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

ROC<- append(ROC, sampling_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_best_df<- ROC_df %>% #Don't need ROC_best_df
  arrange(-ROC) %>%
  top_n(10, ROC)

ROC_best_df %>% knitr::kable(format = "markdown", digits = 3)

best_model<- ROC_best_df$model[which(ROC_best_df$ROC==max(ROC))]
best_model
best_model<- sampling_fits$`rpart - smote`

```



```{r tree, evaluate = FALSE}

# Visualize the decision tree with rpart.plot
rpart.plot(sampling_fits$`rpart - smote`$finalModel, box.palette="RdBu", shadow.col="gray", nn=TRUE)

```


```{r importance, echo = FALSE, evaluate = FALSE}

#load.image(initial_ml)

#Calculate and visualize variable importance
imp <- varImp(best_model)

ggplot(imp)+ggtitle("Model - Variables of importance")

important<- imp$importance %>%
  mutate(Feature = rownames(imp$importance)) %>%
  arrange(-Overall) 

important %>%
  select(Feature, Overall) %>%
  knitr::kable(format = "markdown", digits = 2)

important_vars<- important %>% 
  filter(Overall > 2) %>%
  select(Feature) %>%
  .$Feature

important_vars <- str_remove(string = important_vars, pattern = "\\.[:alpha:]{1}")
important_vars <- append(important_vars, "No_show") #Add dependent variable

```

```{r important_fit, echo = FALSE, evaluate = FALSE}

model<- "rpart"
ctrl$sampling<- NULL
train_df_imp<- train_df[,important_vars]

imp_fits <- lapply(model, function(model){ 
	print(model)
	train(No_show ~ .,
              data = train_df_imp,
              method = model,
	            metric = "ROC",
	            trControl = ctrl)
}) 

names(imp_fits)<- "rpart - no sampling - important vars"

imp_ROC <- sapply(imp_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

ROC<- append(ROC, imp_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_best_df<- ROC_df %>%
  arrange(-ROC) %>%
  top_n(10, ROC)

ROC_best_df %>% knitr::kable(format = "markdown", digits = 3)

best_model<- ROC_best_df$model[which(ROC_best_df$ROC==max(ROC))]
best_model

#best_model<- sampling_fits$`rpart - smote`

```


##Testing different models
To 


```{r models, echo = FALSE, evaluate = FALSE}

#Test different sampling strategies
model<- ("naive_bayes",  "svmLinear", 
                "knn", "kknn",
                "mlp", "monmlp",
                "gbm", "svmRadial", "svmRadialCost", "svmRadialSigma", "svmPoly", "multinom", "avNNet", "glm", "C5.0")
 
sampling_method<- NULL
ctrl$sampling<- sampling_method

model_fits <- lapply(model, function(model){ 
	print(model)
	train(No_show ~ .,
              data = train_df_imp[1:10000,],
              method = model,
	            metric = "ROC",
	            trControl = ctrl)
}) 
    
names(C50_fits) <- paste(model, "- no sampling - important vars")

#Create matrix of accuracy
model_ROC <- sapply(model_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

C50_ROC
ROC<- append(ROC, C50_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  #top_n(10, ROC) %>% 
  knitr::kable(format = "markdown", digits = 3)

best_model<- ROC_df$model[which(ROC_df$ROC==max(ROC))]
best_model

best_model<- model_fits$`gbm no sampling - important vars`

```

###Loading past work

```{r reload, evaluate = FALSE}
list.files(getwd(), pattern = ".Rdata")

load("initial_ml.Rdata")

save_version<- paste("ml-", Sys.Date(), ".Rdata", sep = "")
save_version
save.image(save_version)

load(save_version)


```

```{r tune, evaluate = FALSE}

model<- ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  top_n(2, ROC) %>% 
  select(model) %>%
  mutate(model = word(model, sep = " ")) %>%
  .$model

model<- append(model, "C5.0")
model
 
sampling_method<- "smote"
ctrl$sampling<- sampling_method

tune_fits <- lapply(model, function(model){ 
	print(model)
	train(No_show ~ .,
              data = train_df,
              method = model,
	            metric = "ROC",
	            trControl = ctrl)
}) 

names(tune_fits) <- paste(model, "- smote - full dataset")

#Create matrix of accuracy
tune_ROC <- sapply(tune_fits, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

tune_ROC


ROC<- append(ROC, tune_ROC)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  #top_n(10, ROC) %>% 
  knitr::kable(format = "markdown", digits = 3)
	
```

C5.0 has a steeper curve.

Winnowing seems not to make a difference, tree and rule based model both perform well depending on winnowing or not. 

```{r best_fits_results, evaluate = FALSE}
limits<- c(0.72, 0.78)
ggplot(tune_fits$`C5.0 - smote - full dataset`)+
  scale_y_continuous(limits = limits)
ggplot(tune_fits$`mlp - smote - full dataset`)+
  scale_y_continuous(limits = limits)
ggplot(tune_fits$`gbm - smote - full dataset`)+
  scale_y_continuous(limits = limits)

#grid.arrange(c50_params, mlp_params, gbm_params) # Show C5.0 at top with mlp and gb at the bottom

```



```{r C50_tune, evaluate = FALSE}

getModelInfo("C5.0")
tune_fits$`C5.0 - smote - full dataset`$bestTune

#model
model<- "C5.0"
model

#sampling
sampling_method<- "smote"
ctrl$sampling<- sampling_method

#tuning
grid <- expand.grid(.winnow = c(FALSE), .trials=c(20, 25, 30), .model=c("rules"))

C50_tune <- lapply(model, function(model){ 
	print(model)
	train(No_show ~ .,
              data = train_df,
              method = model,
	            metric = "ROC",
	            trControl = ctrl,
	            tuneGrid = grid)
}) 

names(C50_tune) <- paste(model, "- smote - full dataset - tuning")

ggplot(C50_tune$`C5.0 - smote - full dataset - tuning`)


#Create matrix of accuracy
tune_C50 <- sapply(C50_tune, function(model){ 
	print(model)
	ROC<- mean(model$results$ROC)
	matrix(ROC)

})

tune_C50


ROC<- append(ROC, tune_C50)

ROC_df<- data.frame(ROC)
ROC_df$model<- names(ROC)
rownames(ROC)<- NULL

ROC_df %>% 
  arrange(-ROC) %>%
  select(model, ROC) %>%
  #top_n(10, ROC) %>% 
  knitr::kable(format = "markdown", digits = 3)


```


```{r best, evaluate = FALSE}
load(save_version)


best_models<- list(C50_tune$`C5.0 - smote - full dataset - tuning`,
            tune_fits$`gbm - smote - full dataset`,
            tune_fits$`mlp - smote - full dataset`)

names(best_models)<- c("C5.0", "gbm", "mlp")

  
```



```{r threshold, echo = FALSE, evaluate = FALSE}

#Do this for each model as a function

# Calculate the optimum threshold value and set prediction thresholds

precision_threshold<- .8

prob_thresholds <- sapply(best_models, function(model){ 
print(model)
	#calculate resampling stats
resample_stats <- thresholder(model,threshold = seq(.01, .99, by = 0.04), final = TRUE)

  #Optimize probability threshold
prob_threshold <- resample_stats %>%
  filter(Precision>=precision_threshold) %>%
  filter(Sensitivity==max(Sensitivity)) %>%  
  .$prob_threshold
matrix(prob_threshold)
})

  #Pick out precision and sensitivity from resampled training data
model_stats<- sapply(best_models, function(model){ 
  stats <- thresholder(model,threshold = prob_thresholds[model$method], final = TRUE) %>% 
  select(prob_threshold, Sensitivity, Precision) 
matrix(stats)
})

rownames(model_stats)<- c("prob_threshold", "Sensitivity", "Precision")
model_stats<- data.frame(t(model_stats)) %>%
  mutate(model = colnames(model_stats)) %>%
  mutate(Precision = as.numeric(Precision),
         Sensitivity = as.numeric(Sensitivity)) %>%
  rename(Model = model)


model_stats %>% 
  select(-prob_threshold) %>%
  select(Model, Precision, Sensitivity) %>%
  knitr::kable(format = "markdown", digits = 3)



#Visualize optimal threshold, and trade-off between precision and sensitivity for training set
# ggplot(stats,aes(x=prob_threshold,y=Value))+
#   geom_point(aes(col=Case))+
#   labs(title="Precision/Sensitivity vs Threshold", 
#        y="Precision / Sensitivity", 
#        x="Probability Threshold")+
#   geom_vline(xintercept=pred_threshold, linetype="dotted")


```


```{r metrics, echo = FALSE, evaluate = FALSE}

best_model<- best_models$C5.0
#Predict values and report on model metrics

#Predict values on testing partition based on optimal probability threshold
PRbest <- predict(best_model, test_df, type = "prob")
pred_best <- ifelse(PRbest$No_show >= prob_thresholds["C5.0"], "No_show", "Present") %>% as.factor()


```



```{r roc, echo = FALSE, evaluate = FALSE}
#Plot ROC curve and ROC AUC confidence interval for test partition
load("ml-2020-04-05.Rdata")

#ci.thresholds(rpart.roc, thresholds = "best")
plot.roc(test_df$No_show, PRbest$No_show, print.auc = TRUE, print.thres = "best", main = "C5.0, tuned", legacy.axes = TRUE)

#Call confusion matrix
confusionMatrix(pred_best, test_df$`No_show`, mode = "prec_recall")


```


```{r lift, echo = FALSE, evaluate = FALSE}
#----------------------Calculate lift curves and visualize confusion matrix----------------------

#Create dataframe for lift curves
pr_metric_df<- data.frame(obs = test_df$No_show, prob = PRbest$No_show, pred = pred_best)
# 
# #Calculate and visualize lift curves
# lift<- lift(obs ~ prob, data = pr_metric_df)
# ggplot(lift)+
#   labs(title = "Lift Curve, Undercall Model")

```


```{r confmat, echo = FALSE, evaluate = FALSE}

#Visualize confusion matrix
pr_metric_df %>% 
  mutate(obs=factor(obs, levels=c("No_show","Present"),ordered=TRUE)) %>%
   ggplot(aes(x=obs,y=prob,color=pred))+
   geom_jitter(width = 0.3, height = 0.00, alpha = 0.5)+
     scale_y_continuous(breaks = c(seq.int(0.0, 1, 0.2)))+
     geom_vline(aes(xintercept = 1.5), linetype = "dashed")+
     geom_hline(aes(yintercept = prob_thresholds["C5.0"]), linetype = "dashed")+
     scale_color_manual(values = c("steelblue", "grey45"), name = "Prediction", labels = c("No-Show", "Present"))+
     # geom_label(aes(x = 0.65, y = 0.9), label = str_wrap("True Positives", width = 8), color = "black")+
     # geom_label(aes(x = 0.65, y = 0.1, label = str_wrap("False Negatives", width = 8)), color = "black")+
     # geom_label(aes(x = 2.35, y = 0.1, label = str_wrap("True Negatives", width = 8)), color = "black")+
     # geom_label(aes(x = 2.35, y = 0.9, label = str_wrap("False Positives", width = 8)), color = "black")+
     labs(title = "Confusion Matrix Visual Summary",
          subtitle = "No-show predictions vs. actuals",
          y = "Predicted Probability of No-show",
          x = "Observed Values")


```

## Conclusions
Another iteration could split data according to time, so that the model is trained on appointments happening at the beginning of the timeframe, and the testing is done on appointments at the end of the timeframe. This would avoid the issue of having all patients beginning at 0 past no-shows. However, new patients will always have 0 no-shows. It is likely that predictions on new patients without any history is less accurate than predictions for individuals with many past appointments. 

The presence of negative values for some variables (i. e. time from booking to appointment) meant that they could not undergo log transformation, despite demonstrating a distribution that would lend itself well to said transformation. Access to data without such negative values could potentially improve results. 
